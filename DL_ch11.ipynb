{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 11.1\n",
        "\n",
        "- 모델을 초기화하고 데이터를 로딩한다.\n",
        "- 어느정도 임의로 선택한 에포크 수로 루프를 반복한다.\n",
        "\n",
        "# 11.2 애플리케이션의 메인 진입점"
      ],
      "metadata": {
        "id": "AsOq1srLvBG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from util import importstr\n",
        "from logconf import logging\n",
        "log = logging.getLogger('nb')\n",
        "\n",
        "def run(app, *argv):\n",
        "    argv = list(argv)\n",
        "    argv.insert(0, '--num-workers=4')  # 4코어 8스레드 CPU로 가정\n",
        "    log.info(\"Running: {}({!r}).main()\".format(app, argv))\n",
        "    \n",
        "    app_cls = importstr(*app.rsplit('.', 1)) \n",
        "    app_cls(argv).main()\n",
        "    \n",
        "    log.info(\"Finished: {}.{!r}).main()\".format(app, argv))"
      ],
      "metadata": {
        "id": "thKcosr2vZnE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training.py"
      ],
      "metadata": {
        "id": "4mGpcqUlwA5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "\n",
        "# __init__함수에서 표준 argparse라이브러리를 사용한다. 초기화 함수에 커스텀 인자를 전달한다.\n",
        "\n",
        "class LunaTrainingApp:\n",
        "    def __init__(self, sys_argv=None):\n",
        "        if sys_argv is None:\n",
        "            sys_argv = sys.argv[1:]\n",
        "\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument('--num-workers',\n",
        "            help='Number of worker processes for background data loading',\n",
        "            default=8,\n",
        "            type=int,\n",
        "        )\n",
        "        parser.add_argument('--batch-size',\n",
        "            help='Batch size to use for training',\n",
        "            default=32,\n",
        "            type=int,\n",
        "        )\n",
        "        parser.add_argument('--epochs',\n",
        "            help='Number of epochs to train for',\n",
        "            default=1,\n",
        "            type=int,\n",
        "        )\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "GnVrghQhvw9K"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.3\n",
        "- 모델과 옵티마이저 초기화\n",
        "- Dataset과 DataLoader인스턴스 초기화"
      ],
      "metadata": {
        "id": "R1xUFJPVxQmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install SimpleITK\n",
        "#!pip install diskcache"
      ],
      "metadata": {
        "id": "iXtvvttL3yJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from model import LunaModel\n",
        "from torch.optim import SGD\n",
        "import torch.nn as nn\n",
        "from dsets import LunaDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LunaTrainingApp:\n",
        "    def __init__(self, sys_argv=None):\n",
        "        self.use_cuda = torch.cuda.is_available()\n",
        "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
        "\n",
        "        self.model = self.initModel()\n",
        "        self.optimizer = self.initOptimizer()\n",
        "    ### 모델과 옵티마이저 초기화\n",
        "    def initModel(self):\n",
        "        model = LunaModel()\n",
        "        if self.use_cuda:\n",
        "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
        "            if torch.cuda.device_count() > 1: #복수개의 GPU탐지\n",
        "                model = nn.DataParallel(model) #모델 래핑\n",
        "            model = model.to(self.device) #GPU에 모델 파라미터 전달\n",
        "        return model\n",
        "    \n",
        "    def initOptimizer(self):\n",
        "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
        "        #하이퍼파라미터 탐색을 통해 다른 학습률과 모멘텀값으로 바꿀수 있음\n",
        "\n",
        "\n",
        "    ### Dataset과 DataLoader인스턴스 초기화\n",
        "    def initTrainDl(self):\n",
        "        train_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=False,\n",
        "        )\n",
        "\n",
        "        batch_size = self.cli_args.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        train_dl = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=batch_size, #알아서 배치로 나뉜다.\n",
        "            num_workers=self.cli_args.num_workers,\n",
        "            pin_memory=self.use_cuda, #고정된 메모리 영역이 GPU쪽으로 빠르게 전송됨\n",
        "        )\n",
        "\n",
        "        return train_dl\n",
        "\n",
        "\n",
        "    def initValDl(self):\n",
        "        val_ds = LunaDataset(\n",
        "            val_stride=10,\n",
        "            isValSet_bool=True, #train과 똑같이 하는데 여기만 다르게 해주면 됨.\n",
        "        )\n",
        "\n",
        "        batch_size = self.cli_args.batch_size\n",
        "        if self.use_cuda:\n",
        "            batch_size *= torch.cuda.device_count()\n",
        "\n",
        "        val_dl = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=self.cli_args.num_workers,\n",
        "            pin_memory=self.use_cuda,\n",
        "        )\n",
        "\n",
        "##나중에 메인함수에서 할당만 시키면 됨.\n",
        "    def main(self):\n",
        "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
        "\n",
        "        train_dl = self.initTrainDl()\n",
        "        val_dl = self.initValDl()"
      ],
      "metadata": {
        "id": "2JWJaYvJ3nwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.4\n",
        "- 테일: 입력을 신경망에 넣기 전 처리과정을 담달하는 제일 첫 부분의 일부 계층\n",
        "  - 백본이 원하는 형태로 입력을 만들어야 하므로 신경망의 나머지 부분과는 구조나 구성이 다른 경우가 많다. 테일에 컨볼루션 층이 들어간 경우는 이미지 크기를 공격적으로 다운 샘플링하기 위한 용도가 대부분이다.\n",
        "\n",
        "- 백본: 여러계층을 가지는데 일반적으로는 연속된 블럭에 배치된다.\n",
        "  - 각 블럭은 동일한 세트의 계층을 가지며 블럭과 블럭을 거칠 때마다 필요한 입력크기나 필터 수가 달라진다.\n",
        "  - 여기에는 3x3컨볼루션 두개와 하나의 활성화 함수 그리고 블록 끝에 맥스풀링 연산이 이어진다.\n",
        "- 헤드: 백본의 출력을 받아 원하는 출력 형태로 바꾼다. 컨볼루션 신경망에서 이 작업을 중간 출력물을 flatten하고 완전 연결계층에 전달하는 역할을 하기도 한다."
      ],
      "metadata": {
        "id": "uo2Gwim-30Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#블럭은 아래처럼\n",
        "# (컨볼루션-> 활성화함수-> 컨볼루션-> 활성화함수-> 맥스풀링)인 블럭을 정의\n",
        "class LunaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, conv_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
        "        )\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.maxpool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        block_out = self.conv1(input_batch)\n",
        "        block_out = self.relu1(block_out)\n",
        "        block_out = self.conv2(block_out)\n",
        "        block_out = self.relu2(block_out)\n",
        "\n",
        "        return self.maxpool(block_out)"
      ],
      "metadata": {
        "id": "vBxB2LFA2vQG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#전체모델\n",
        "class LunaModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, conv_channels=8):\n",
        "        super().__init__()\n",
        "\n",
        "        ##테일 부분, 이 모델은 이미지가 충분히 작아 다운샘플링없이 배치정규화만 구성.\n",
        "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
        "\n",
        "\n",
        "        #백본: 위에 정의한 블록을 4번 쌓아올림\n",
        "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
        "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
        "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
        "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
        "\n",
        "\n",
        "        #헤드: 백본의 출력을 fc층과 연결시킨 후 softmax통과\n",
        "        self.head_linear = nn.Linear(1152, 2)\n",
        "        self.head_softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self._init_weights()\n",
        "    \n",
        "    #파라미터 초기화부분\n",
        "    #1보다 작은 가중치는 잔차연결이 없는 경우 매우 좋지 않은 성능을 낼 수 있다. 때문에 가중치 초기화부분도 모델의 성능에 영향을 준다.\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if type(m) in {\n",
        "                nn.Linear,\n",
        "                nn.Conv3d,\n",
        "                nn.Conv2d,\n",
        "                nn.ConvTranspose2d,\n",
        "                nn.ConvTranspose3d,\n",
        "            }:\n",
        "                nn.init.kaiming_normal_(\n",
        "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
        "                )\n",
        "                if m.bias is not None:\n",
        "                    fan_in, fan_out = \\\n",
        "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
        "                    bound = 1 / math.sqrt(fan_out)\n",
        "                    nn.init.normal_(m.bias, -bound, bound)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        bn_output = self.tail_batchnorm(input_batch)\n",
        "\n",
        "        block_out = self.block1(bn_output)\n",
        "        block_out = self.block2(block_out)\n",
        "        block_out = self.block3(block_out)\n",
        "        block_out = self.block4(block_out)\n",
        "\n",
        "        #평탄화 작업을 해야함.\n",
        "        conv_flat = block_out.view( \n",
        "            block_out.size(0), #배치크기\n",
        "            -1,\n",
        "        )\n",
        "        linear_output = self.head_linear(conv_flat)\n",
        "\n",
        "        #반환값은 로지트와 소프트맥스 확룰을 만든다. 로지트는 손실함수 계산을 위해 사용되고 소프트맥스 확률은 실제 분류시 확률값을 이용한다.\n",
        "        return linear_output, self.head_softmax(linear_output)"
      ],
      "metadata": {
        "id": "1-9NWFbW4x97"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.5"
      ],
      "metadata": {
        "id": "G3rnjohr7Qrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련\n",
        "\n",
        "METRICS_SIZE = 3\n",
        "from util import enumerateWithEstimate\n",
        "\n",
        "class LunaTrainingApp:\n",
        "  def main(self):\n",
        "\n",
        "  #에포크마다 훈련 결과를 남기기\n",
        "    for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
        "      trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
        "      self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
        "\n",
        "  def doTraining(self, epoch_ndx, train_dl):\n",
        "        self.model.train()\n",
        "\n",
        "        #빈 메트릭 배열을 초기화\n",
        "        trnMetrics_g = torch.zeros(\n",
        "            METRICS_SIZE,\n",
        "            len(train_dl.dataset),\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        #시간을 예측하며 배치루프를 설정한다.\n",
        "        batch_iter = enumerateWithEstimate(\n",
        "            train_dl,\n",
        "            \"E{} Training\".format(epoch_ndx),\n",
        "            start_ndx=train_dl.num_workers,\n",
        "        )\n",
        "        for batch_ndx, batch_tup in batch_iter:\n",
        "            self.optimizer.zero_grad() #그라디언트 0으로 초기화\n",
        "\n",
        "\n",
        "            #손실함수 계산\n",
        "            loss_var = self.computeBatchLoss(\n",
        "                batch_ndx,\n",
        "                batch_tup,\n",
        "                train_dl.batch_size,\n",
        "                trnMetrics_g\n",
        "            )\n",
        "\n",
        "            loss_var.backward()\n",
        "\n",
        "            #파라미터 업데이트\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
        "\n",
        "        return trnMetrics_g.to('cpu')"
      ],
      "metadata": {
        "id": "W2_cYq2y5X2_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS_LABEL_NDX=0\n",
        "METRICS_PRED_NDX=1\n",
        "METRICS_LOSS_NDX=2\n",
        "\n",
        "class LunaTrainingApp:\n",
        "\n",
        "  #훈련루프와 검증루프 둘다에서 호출될 함수이다. 샘플배치에 대한 손실을 계산하고, 각 샘플에 대해 모델이 만들어내는 출력 정보도 기록한다. \n",
        "  #이를 통해 각 클래스별로 계산이 얼마나 정확한지 백분율로 알아보고 분류가 잘 되지 않는 클래스를 찾아 집중개선 가능하다.\n",
        "  def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
        "        input_t, label_t, _series_list, _center_list = batch_tup\n",
        "\n",
        "        input_g = input_t.to(self.device, non_blocking=True) #GPU로 이동\n",
        "        label_g = label_t.to(self.device, non_blocking=True) #GPU로 이동\n",
        "\n",
        "        logits_g, probability_g = self.model(input_g) #모델의 출력값을 저장\n",
        "\n",
        "        loss_func = nn.CrossEntropyLoss(reduction='none') #샘플별 손실값을 얻음\n",
        "        loss_g = loss_func(logits_g,label_g[:,1]) #손실값: logits_g와 원핫인코딩 클래스의 인덱스인 label_g[:,1]사이의 로스\n",
        "\n",
        "        #추후 분석을 위해 샘플별 통계값 기록\n",
        "        start_ndx = batch_ndx * batch_size\n",
        "        end_ndx = start_ndx + label_t.size(0)\n",
        "\n",
        "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = label_g[:,1].detach()\n",
        "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = probability_g[:,1].detach()\n",
        "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g.detach()\n",
        "\n",
        "        return loss_g.mean()"
      ],
      "metadata": {
        "id": "MvoEvmME9OWZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 검증루프\n",
        "class LunaTrainingApp:\n",
        "  def main(self):\n",
        "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
        "\n",
        "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
        "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
        "\n",
        "  def doValidation(self, epoch_ndx, val_dl):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval() #훈련때 사용했던 기능은 끈다.\n",
        "            valMetrics_g = torch.zeros(\n",
        "                METRICS_SIZE,\n",
        "                len(val_dl.dataset),\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "            batch_iter = enumerateWithEstimate(\n",
        "                val_dl,\n",
        "                \"E{} Validation \".format(epoch_ndx),\n",
        "                start_ndx=val_dl.num_workers,\n",
        "            )\n",
        "            for batch_ndx, batch_tup in batch_iter:\n",
        "                self.computeBatchLoss(\n",
        "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
        "\n",
        "        return valMetrics_g.to('cpu')"
      ],
      "metadata": {
        "id": "Gbl9op0y_a_a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.6\n",
        "각 에포크마다 진행사항은 trnMetrics_g와 valMetrics_g에 수집했다. 또한 텐서에는 훈련과 검증단계에 대한 클래스별로 백분율이나 평균손실값을 계산하기 위해 모든 데이터가 포함되어 있으므로 이를 통해 성능을 알아볼 수 있다."
      ],
      "metadata": {
        "id": "r22PrqEi_7ny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def logMetrics(\n",
        "            self,\n",
        "            epoch_ndx, #결과를 로깅할때 인덱스를 표시하는 용도\n",
        "            mode_str, #메트릭이 훈련용인지 검증용인지 표시\n",
        "            metrics_t, #결과를 담고있는 용도\n",
        "            classificationThreshold=0.5):\n",
        "  \n",
        "  #마스크 구성: 결절샘플또는 비결절샘플에 대해서만 메트릭을 제한하는 마스크이다. 또한 클래스별 총 샘플 수와 잘 분류된 샘플 수를 알아볼 수 있다.\n",
        "  negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold #실제값들 중 0.5보다 작은 것들에 대해서 불리언값으로 가지고있음\n",
        "  negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold #예측값들 중 0.5보다 작은 것들에 대해서 불리언값으로 가지고있음\n",
        "\n",
        "  posLabel_mask = ~negLabel_mask #실제값들 중 0.5보다 큰 것들에 대해서 불리언값으로 가지고있음\n",
        "  posPred_mask = ~negPred_mask #예측값들 중 0.5보다 큰 것들에 대해서 불리언값으로 가지고있음\n",
        "\n",
        "  neg_count = int(negLabel_mask.sum()) #실제값들중 음성인 개수\n",
        "  pos_count = int(posLabel_mask.sum()) #실제값들중 양성인 개수\n",
        "\n",
        "  neg_correct = int((negLabel_mask & negPred_mask).sum()) #음성으로 맞게 분류한 것들의 개수\n",
        "  pos_correct = int((posLabel_mask & posPred_mask).sum()) #양성으로 맞게 분류한 것들의 개수\n",
        "\n",
        "  #계산한 값들 전부 딕셔너리에 저장\n",
        "  metrics_dict = {}\n",
        "  metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
        "  metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
        "  metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
        "\n",
        "  metrics_dict['correct/all'] = (pos_correct + neg_correct)/np.float32(metrics_t.shape[1]) * 100\n",
        "  metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
        "  metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
        "\n",
        "  #저장한 결과 로깅\n",
        "  log.info(\n",
        "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
        "                 + \"{correct/all:-5.1f}% correct, \"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "  log.info(\n",
        "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
        "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_neg',\n",
        "                neg_correct=neg_correct,\n",
        "                neg_count=neg_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "  log.info(\n",
        "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
        "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
        "            ).format(\n",
        "                epoch_ndx,\n",
        "                mode_str + '_pos',\n",
        "                pos_correct=pos_correct,\n",
        "                pos_count=pos_count,\n",
        "                **metrics_dict,\n",
        "            )\n",
        "        )\n",
        "  \n"
      ],
      "metadata": {
        "id": "2aj8H9Ex_3oh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.7\n"
      ],
      "metadata": {
        "id": "f199Y4LIDhD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "for i,_ in enumerateWithEstimate(list(range(234)),\"training\"):\n",
        "  time.sleep(random.random())\n",
        "#해당 함수를 이용해 프로젝트의 훈련시간이 어느정도 걸리는지 예측 가능하다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QpxCF50AkCV",
        "outputId": "2fc8e022-4ac7-4a2d-f357-89c211e847d7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-16 06:51:21,421 WARNING  pid:57 util:209:enumerateWithEstimate training ----/234, starting\n",
            "2022-09-16 06:51:23,642 INFO     pid:57 util:229:enumerateWithEstimate training    4/234, done at 2022-09-16 06:53:04, 0:01:43\n",
            "2022-09-16 06:51:28,922 INFO     pid:57 util:229:enumerateWithEstimate training   16/234, done at 2022-09-16 06:53:04, 0:01:43\n",
            "2022-09-16 06:51:52,897 INFO     pid:57 util:229:enumerateWithEstimate training   64/234, done at 2022-09-16 06:53:14, 0:01:53\n",
            "2022-09-16 06:53:19,785 WARNING  pid:57 util:240:enumerateWithEstimate training ----/234, done at 2022-09-16 06:53:19\n"
          ]
        }
      ]
    }
  ]
}